{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "from pandapower import runpp\n",
    "from pandapower.plotting import simple_plotly, pf_res_plotly\n",
    "import pandapower.networks as networks\n",
    "from citylearn import CityLearn\n",
    "from gridlearn import GridLearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "climate_zone = 1\n",
    "data_path = Path(\"data/Climate_Zone_\"+str(climate_zone))\n",
    "building_attributes = data_path / 'building_attributes.json'\n",
    "weather_file = data_path / 'weather_data.csv'\n",
    "solar_profile = data_path / 'solar_generation_1kW.csv'\n",
    "building_state_actions = 'buildings_state_action_space.json'\n",
    "building_ids = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\",\"Building_10\"]\n",
    "# building_ids = [\"Building_1\",\"Building_4\",\"Building_5\",\"Building_6\"] # only building types with pv\n",
    "objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption', 'system_losses']\n",
    "my_grid = GridLearn(data_path, building_attributes, weather_file, solar_profile, building_ids, 6, buildings_states_actions = building_state_actions, cost_function = objective_function, verbose=1, n_buildings_per_bus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_spaces, actions_spaces = my_grid.get_state_action_spaces()\n",
    "\n",
    "# Simulation without energy storage\n",
    "my_grid.reset()\n",
    "done = False\n",
    "# while not done:\n",
    "for ts in range(6):\n",
    "    # for the do nothing action:\n",
    "    _, rewards, done, _ = my_grid.step([[0 for _ in range(actions_spaces[i].shape[0])] for i in range(len(my_grid.buildings))])\n",
    "    \n",
    "#     # for randomized actions:\n",
    "#     _, rewards, done, _ = my_grid.step([actions_spaces[i].sample() for i in range(len(my_grid.buildings))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pf_res_plotly(my_grid.net)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_grid.plot_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid.cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the grid...\n",
      "Initializing the agents...\n",
      "Starting the experiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn.py:560: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state.append(np.array(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulated reward: 20.67531443364198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn.py:560: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state.append(np.array(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - {'ramping': 0.029705783, '1-load_factor': 0.09900749118339175, 'average_daily_peak': 0.16063409, 'peak_demand': 0.16063409, 'net_electricity_consumption': 0.3178293, 'system_losses': 0.3157936940038272, 'quadratic': 0.06935744, 'total': 0.16470884148301376} Simulation time (min) - 1.296626083056132\n",
      "Cumulated reward: 20.615952785804332\n",
      "Loss - {'ramping': 0.031054115, '1-load_factor': 0.13040818277411337, 'average_daily_peak': 0.16418128, 'peak_demand': 0.16418128, 'net_electricity_consumption': 0.31924835, 'system_losses': 0.3172644005620347, 'quadratic': 0.07006589, 'total': 0.17091478489687237} Simulation time (min) - 1.623163398106893\n",
      "Cumulated reward: 20.261830500340555\n",
      "Loss - {'ramping': 0.054566707, '1-load_factor': 0.17830275783391633, 'average_daily_peak': 0.16284174, 'peak_demand': 0.16284174, 'net_electricity_consumption': 0.30817264, 'system_losses': 0.3061806387596862, 'quadratic': 0.06545615, 'total': 0.17690891053476107} Simulation time (min) - 1.9505075136820476\n",
      "Cumulated reward: 21.17400903662392\n",
      "Loss - {'ramping': 0.039688498, '1-load_factor': 0.15191961932172363, 'average_daily_peak': 0.16359009, 'peak_demand': 0.16359009, 'net_electricity_consumption': 0.31427664, 'system_losses': 0.3122286120748075, 'quadratic': 0.06814537, 'total': 0.17334841625313743} Simulation time (min) - 2.2853092511494952\n"
     ]
    }
   ],
   "source": [
    "# Run this again after editing submodules so Colab uses the updated versions\n",
    "from citylearn import CityLearn\n",
    "from gridlearn import GridLearn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from agent import RL_Agents_Coord\n",
    "import numpy as np                                                                                                                                                                                      \n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "\n",
    "# Load environment\n",
    "climate_zone = 1\n",
    "data_path = Path(\"data/Climate_Zone_\"+str(climate_zone))\n",
    "building_attributes = data_path / 'building_attributes.json'\n",
    "weather_file = data_path / 'weather_data.csv'\n",
    "solar_profile = data_path / 'solar_generation_1kW.csv'\n",
    "building_state_actions = 'buildings_state_action_space.json'\n",
    "building_id = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
    "objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption','quadratic','system_losses']\n",
    "\n",
    "print(\"Initializing the grid...\")\n",
    "# Contain the lower and upper bounds of the states and actions, to be provided to the agent to normalize the variables between 0 and 1.\n",
    "# Can be obtained using observations_spaces[i].low or .high\n",
    "env = GridLearn(data_path, building_attributes, weather_file, solar_profile, building_id, 6, buildings_states_actions = building_state_actions, simulation_period = (0,4), cost_function = objective_function, verbose=1, n_buildings_per_bus=1)\n",
    "\n",
    "# Hyperparameters\n",
    "bs = 256\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "lr = 0.0003\n",
    "hid = [256,256]\n",
    "\n",
    "n_episodes = 4\n",
    "\n",
    "print(\"Initializing the agents...\")\n",
    "# Instantiating the control agent(s)\n",
    "agents = RL_Agents_Coord(env, discount = gamma, batch_size = bs, replay_buffer_capacity = 1e5, regression_buffer_capacity = 12*8760, tau=tau, lr=lr, hidden_dim=hid, start_training=8760*3, exploration_period = 8760*3+1,  start_regression=8760, information_sharing = True, pca_compression = .95, action_scaling_coef=0.5, reward_scaling = 5., update_per_step = 1, iterations_as = 2)\n",
    "\n",
    "print(\"Starting the experiment...\")\n",
    "# The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\n",
    "start = time.time()\n",
    "for e in range(n_episodes): \n",
    "    is_evaluating = (e > 7) # Evaluate deterministic policy after 7 epochs\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    j = 0\n",
    "    action, coordination_vars = agents.select_action(state, deterministic=is_evaluating)    \n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        action_next, coordination_vars_next = agents.select_action(next_state, deterministic=is_evaluating)\n",
    "        agents.add_to_buffer(state, action, reward, next_state, done, coordination_vars, coordination_vars_next)\n",
    "\n",
    "        state = next_state\n",
    "        coordination_vars = coordination_vars_next\n",
    "        action = action_next\n",
    "\n",
    "    print('Loss -',env.cost(), 'Simulation time (min) -',(time.time()-start)/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.785291186515646, 0.8366420820418948, 0.7307344021784503, 0.673332128442126]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.system_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(env.buildings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda",
   "language": "python",
   "name": "panda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
