{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the agents...\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# Run this again after editing submodules so Colab uses the updated versions\n",
    "from citylearn import CityLearn\n",
    "from citylearn import GridLearn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from citylearn import RL_Agents_Coord, Cluster_Agents\n",
    "import numpy as np                                                                                                                                                                                      \n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from joblib import dump, load\n",
    "\n",
    "# Load environment\n",
    "climate_zone = 1\n",
    "data_path = Path(\"citylearn/data/Climate_Zone_\"+str(climate_zone))\n",
    "building_attributes = data_path / 'building_attributes.json'\n",
    "weather_file = data_path / 'weather_data.csv'\n",
    "solar_profile = data_path / 'solar_generation_1kW.csv'\n",
    "building_state_actions = 'citylearn/buildings_state_action_space.json'\n",
    "building_id = [\"Building_1\",\"Building_2\",\"Building_3\",\"Building_4\",\"Building_5\",\"Building_6\",\"Building_7\",\"Building_8\",\"Building_9\"]\n",
    "objective_function = ['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption','quadratic','voltage_dev']\n",
    "\n",
    "ep_period = 10\n",
    "\n",
    "print(\"Initializing the grid...\")\n",
    "# Contain the lower and upper bounds of the states and actions, to be provided to the agent to normalize the variables between 0 and 1.\n",
    "# Can be obtained using observations_spaces[i].low or .high\n",
    "env = GridLearn(data_path, building_attributes, weather_file, solar_profile, building_id, 1, buildings_states_actions = building_state_actions, simulation_period = (0,ep_period), cost_function = objective_function, verbose=1, n_buildings_per_bus=1)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 254\n",
    "bs = batch_size\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "lr = 0.0003\n",
    "hid = [batch_size,batch_size]\n",
    "\n",
    "n_episodes = 5\n",
    "n_training_eps = n_episodes - 1\n",
    "\n",
    "if not (batch_size < ep_period * n_training_eps):\n",
    "    print(\"will produce a key error because the neural nets won't be initialized yet\")\n",
    "\n",
    "print(\"Initializing the agents...\")\n",
    "# Instantiating the control agent(s)\n",
    "agents = RL_Agents_Coord(env, list(env.buildings.keys()), discount = gamma, batch_size = bs, replay_buffer_capacity = 1e5, regression_buffer_capacity = 12*ep_period, tau=tau, lr=lr, hidden_dim=hid, start_training=(ep_period+1)*(n_episodes-1), exploration_period = (ep_period+1)*(n_episodes)+1,  start_regression=(ep_period+1), information_sharing = True, pca_compression = .95, action_scaling_coef=0.5, reward_scaling = 5., update_per_step = 1, iterations_as = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the experiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_deterministic False\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Cumulated reward: 19.81360799389357\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n",
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss - {'ramping': 1.0649052339710097, '1-load_factor': 0.8042406467634421, 'average_daily_peak': 1.7071949229751466, 'peak_demand': 1.7071949229751466, 'net_electricity_consumption': 1.6352639686566997, 'voltage_dev': 1.5247893862922681, 'quadratic': 2.6770741568910794, 'total': 1.588666176932113} Simulation time (min) - 0.5289287010828654\n",
      "is_deterministic False\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "Cumulated reward: 21.766367737180445\n",
      "22\n",
      "Loss - {'ramping': 1.2636191884306702, '1-load_factor': 0.697389730411083, 'average_daily_peak': 1.6151286444670008, 'peak_demand': 1.6151286444670008, 'net_electricity_consumption': 1.690290301598393, 'voltage_dev': 1.6528219741693588, 'quadratic': 2.8503065975967985, 'total': 1.6263835830200435} Simulation time (min) - 0.7063748995463054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_deterministic False\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "Cumulated reward: 22.148975578151706\n",
      "33\n",
      "Loss - {'ramping': 1.2590716864080573, '1-load_factor': 0.7016823741736532, 'average_daily_peak': 1.6303569842428975, 'peak_demand': 1.6303569842428975, 'net_electricity_consumption': 1.6505671718217774, 'voltage_dev': 1.6614692965645055, 'quadratic': 2.728320408437959, 'total': 1.6088321294131067} Simulation time (min) - 0.8638240536053975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_deterministic False\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "Cumulated reward: 20.60639277789683\n",
      "44\n",
      "Loss - {'ramping': 1.1685638181405889, '1-load_factor': 0.7594803396301438, 'average_daily_peak': 1.7273694926026353, 'peak_demand': 1.7273694926026353, 'net_electricity_consumption': 1.6729339577242317, 'voltage_dev': 1.5874456110880018, 'quadratic': 2.809743776556291, 'total': 1.6361294983349326} Simulation time (min) - 1.0325966676076253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aislingpigott/Documents/CityLearn/citylearn/citylearn.py:666: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.state = np.array(self.state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_deterministic False\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "normalizing...\n",
      "(35, 37)\n",
      "52\n",
      "53\n",
      "54\n",
      "Cumulated reward: 21.30036001440004\n",
      "55\n",
      "Loss - {'ramping': 0.9889972643140278, '1-load_factor': 0.6925399084788022, 'average_daily_peak': 1.6665008845105223, 'peak_demand': 1.6665008845105223, 'net_electricity_consumption': 1.6947113985925515, 'voltage_dev': 1.6434420796963138, 'quadratic': 2.8589285239119357, 'total': 1.6016601348592394} Simulation time (min) - 1.2173382997512818\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the experiment...\")\n",
    "# The number of episodes can be replaces by a stopping criterion (i.e. convergence of the average reward)\n",
    "start = time.time()\n",
    "for e in range(n_episodes):\n",
    "    is_evaluating = (e > n_training_eps) # Evaluate deterministic policy after 7 epochs\n",
    "    rewards = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    j = 0\n",
    "    \n",
    "    print(\"is_deterministic\", is_evaluating)\n",
    "    action, coordination_vars = agents.select_action(state, deterministic=is_evaluating)\n",
    "#     print(action)\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        action_next, coordination_vars_next = agents.select_action(next_state, deterministic=is_evaluating)\n",
    "        agents.add_to_buffer(state, action, reward, next_state, done, coordination_vars, coordination_vars_next)\n",
    "\n",
    "        state = next_state\n",
    "        coordination_vars = coordination_vars_next\n",
    "        action = action_next\n",
    "\n",
    "    print('Loss -',env.cost(), 'Simulation time (min) -',(time.time()-start)/60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [j[0] for j in agents.replay_buffer['JTMN9'].buffer]\n",
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "from stable_baselines3 import SAC\n",
    "from citylearn import  CityLearn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from gridlearn import GridLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs import register\n",
    "\n",
    "climate_zone = 1\n",
    "data_path = Path(\"data/Climate_Zone_\"+str(climate_zone))\n",
    "config = {\n",
    "    'data_path': data_path,\n",
    "    'building_attributes': data_path / 'building_attributes.json',\n",
    "    'buildings_states_actions': 'buildings_state_action_space.json',\n",
    "    'weather_file': data_path / 'weather_data.csv',\n",
    "    'solar_profile': data_path / 'solar_generation_1kW.csv',\n",
    "    'building_ids': ['Building_3'],\n",
    "    'hourly_timesteps': 3,\n",
    "    'central_agent':True,\n",
    "    'cost_function':['ramping','1-load_factor','average_daily_peak','peak_demand','net_electricity_consumption','quadratic']\n",
    "}\n",
    "\n",
    "env_name = 'MyEnv-v1'\n",
    "\n",
    "register(id=env_name,\n",
    "     entry_point='gridlearn:GridLearn',\n",
    "     max_episode_steps=8760,\n",
    "     kwargs = config)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SAC(MlpPolicy, env, verbose=0, learning_rate=0.01, gamma=0.99, tau=3e-4, batch_size=64, learning_starts=8759)\n",
    "start = time.time()\n",
    "print(\"starting learning\")\n",
    "model.learn(total_timesteps=20, log_interval=1000)\n",
    "print(time.time()-start)\n",
    "\n",
    "obs = env.reset()\n",
    "dones = False\n",
    "counter = []\n",
    "print(\"starting evaluation\")\n",
    "while dones==False:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    counter.append(rewards)\n",
    "env.cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.n_buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type({\"a\":1}) ==dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda",
   "language": "python",
   "name": "panda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
